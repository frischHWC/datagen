#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#  http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#
# WARNING: These two properties are required to make the spring boot work with all dependencies
server.tomcat.additional-tld-skip-patterns=*.jar
spring.mvc.pathmatch.matching-strategy=ANT_PATH_MATCHER

# General conf that should be set when starting the program
server.port=4242
app.name=Data Generator
hadoop.user=datagen
hadoop.home=/user/datagen

# General default Conf
threads=4
number.batches.default=20
number.rows.default=20

datagen.home.directory=/tmp/datagen
datagen.model.path=src/main/resources/test-models/
datagen.model.received.path=#{datagen.home.directory}/models-received
datagen.model.generated.path=#{datagen.home.directory}/models-generated
datagen.model.default=full-model.json
datagen.custom.model=
datagen.scheduler.file.path=#{datagen.home.directory}/scheduler/commands.txt

# TLS settings
ssl.server.enabled=false
# Un-comment and fill-in below properties to enable TLS (and set above property to true)
#server.ssl.key-store=dev-support/test_files/keystore.jks
#server.ssl.key-store-password=Test1234
#server.ssl.key-store-type=JKS
#server.ssl.key-alias=test-keystore
#server.ssl.key-password=Test1234
#server.ssl.trust-store=dev-support/test_files/truststore.jks
#server.ssl.trust-store-password=Test1234

# Auth config
security.basic.enabled=false
admin.user=admin
admin.password=admin

# Kerberos config
kerberos.enabled=
kerberos.user=datagen
kerberos.keytab=datagen.keytab

# TLS Config
tls.enabled=
truststore.location=
truststore.password=
keystore.location=
keystore.password=

# These are default config file's path in a CDP cluster
# In case below settings are not set, these files are used to extrapolate them
hadoop.core.site.path=
hadoop.hdfs.site.path=
hadoop.ozone.site.path=
hadoop.hbase.site.path=
hadoop.hive.site.path=
solr.env.path=
kafka.conf.client.path=
schema.registry.conf.path=
kudu.conf.path=

# Auto-discovery Using CM - This replaces config files if it is set
cm.autodiscovery=false
cm.url=
cm.user=
cm.password=
cm.cluster.name=

# Other configurations are either derived from previous or set using APIs
# HDFS
hdfs.uri=
hdfs.auth.kerberos=#{kerberos.enabled}
# It is not needed to fill below configuration if KERBEROS is not activated
hdfs.auth.kerberos.user=#{kerberos.user}
hdfs.auth.kerberos.keytab=#{kerberos.keytab}


# HBASE
hbase.zookeeper.quorum=
hbase.zookeeper.port=
hbase.zookeeper.znode=
hbase.auth.kerberos=#{kerberos.enabled}
# It is not needed to fill below configuration if KERBEROS is not activated
hbase.security.user=#{kerberos.user}
hbase.security.keytab=#{kerberos.keytab}


# OZONE
ozone.service.id=
ozone.auth.kerberos=#{kerberos.enabled}
# It is not needed to fill below configuration if KERBEROS is not activated
ozone.auth.kerberos.user=#{kerberos.user}
ozone.auth.kerberos.keytab=#{kerberos.keytab}


# HIVE
hive.zookeeper.quorum=
hive.zookeeper.znode=
hive.auth.kerberos=#{kerberos.enabled}
# It is not needed to fill below configuration if KERBEROS is not activated
hive.security.user=#{kerberos.user}
hive.security.keytab=#{kerberos.keytab}
hive.truststore.location=#{truststore.location}
hive.truststore.password=#{truststore.password}


# SOLR
solr.zookeeper.quorum=
solr.zookeeper.znode=
solr.tls.enabled=#{tls.enabled}
solr.auth.kerberos=#{kerberos.enabled}
# It is not needed to fill below configuration if KERBEROS is not activated
solr.auth.kerberos.keytab=#{kerberos.keytab}
solr.auth.kerberos.user=#{kerberos.user}
solr.truststore.location=#{truststore.location}
solr.truststore.password=#{truststore.password}
solr.keystore.location=#{keystore.location}
solr.keystore.password=#{keystore.password}


# KAFKA
kafka.brokers=
kafka.security.protocol=
schema.registry.url=
schema.registry.tls.enabled=#{tls.enabled}
# It is not needed to fill below configuration if KERBEROS is not activated (i.e. protocol is PLAINTEXT)
kafka.keystore.location=#{keystore.location}
kafka.truststore.location=#{truststore.location}
kafka.keystore.password=#{keystore.password}
kafka.keystore.key.password=#{keystore.password}
kafka.truststore.password=#{truststore.password}
kafka.sasl.mechanism=
kafka.sasl.kerberos.service.name=
kafka.auth.kerberos.keytab=#{kerberos.keytab}
kafka.auth.kerberos.user=#{kerberos.user}


# KUDU
kudu.master.server=
kudu.auth.kerberos=#{kerberos.enabled}
# It is not needed to fill below configuration if KERBEROS is not activated
kudu.security.user=#{kerberos.user}
kudu.security.keytab=#{kerberos.keytab}
kudu.truststore.location=#{truststore.location}
kudu.truststore.password=#{truststore.password}

# S3
s3.access_key.id=
s3.access_key.secret=
s3.region=

# Azure
adls.account.name=
# Type can be either dfs or blob
adls.account.type=dfs
adls.sas.token=

# GCS
gcs.project.id=
# Only if using a service account key, otherwise use any other ADC login
gcs.accountkey.path=
gcs.region=